{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "import socketserver\n",
    "import pickle\n",
    "import urllib.request\n",
    "import json\n",
    "from pprint import pprint\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from math import sqrt\n",
    "import os\n",
    "import errno\n",
    "from pymongo import MongoClient\n",
    "import urllib.parse as urlparse\n",
    "from influxdb import InfluxDBClient\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllNodeNames(client):\n",
    "    queryResult = client.query(\"SHOW TAG VALUES FROM uptime WITH KEY=nodename;\")\n",
    "    nodeNames_temp = list(queryResult.get_points())\n",
    "    dfnodeNames = pd.DataFrame(nodeNames_temp)\n",
    "    allNodeNames = dfnodeNames[:][\"value\"]\n",
    "    return allNodeNames\n",
    "def getNamespaceNames(client,node):\n",
    "    nsQuery = client.query(\"SHOW TAG VALUES FROM uptime WITH KEY=namespace_name WHERE nodename = '\"+node+\"';\")\n",
    "    nsQuery_temp = list(nsQuery.get_points())\n",
    "    dfnsNames = pd.DataFrame(nsQuery_temp)\n",
    "    allnsNames = dfnsNames[:][\"value\"]\n",
    "    return allnsNames\n",
    "def getAllPodNames(client,node,ns_name):\n",
    "    queryResult = client.query(\"SHOW TAG VALUES FROM uptime WITH KEY = pod_name WHERE namespace_name = '\"+ns_name+\"' AND nodename = '\"+node+\"';\")\n",
    "    podNames_temp = list(queryResult.get_points())\n",
    "    dfpodNames = pd.DataFrame(podNames_temp)\n",
    "    if dfpodNames.empty:\n",
    "        return dfpodNames\n",
    "    else:\n",
    "        allpodNames = dfpodNames[:][\"value\"]\n",
    "        return allpodNames\n",
    "def getCPUUtilizationNode(client, node):\n",
    "    queryResult = client.query('SELECT * FROM \"cpu/node_utilization\" where nodename = \\''+node+'\\' AND type=\\'node\\';')\n",
    "    dfcpuUtilization = pd.DataFrame(queryResult['cpu/node_utilization'])\n",
    "    return dfcpuUtilization\n",
    "def getCPUUtilizationPod(client, node,ns_name, pod_name):\n",
    "    queryResult = client.query('SELECT * FROM \"cpu/usage_rate\" where nodename = \\''+node+'\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+'\\'  AND type=\\'pod\\';')\n",
    "    dfcpuUtilization = pd.DataFrame(queryResult['cpu/usage_rate'])\n",
    "    return dfcpuUtilization\n",
    "def getCPUUtilizationPodContainer(client,node,ns_name, pod_name):\n",
    "    queryResult = client.query('SELECT * FROM \"cpu/usage_rate\" where nodename = \\''+node+'\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+'\\' AND type=\\'pod_container\\';')\n",
    "    dfcpuUtilization = pd.DataFrame(queryResult['cpu/usage_rate'])\n",
    "    return dfcpuUtilization\n",
    "def prepareCpuUtilization(client,node,ns_name, pod_name):\n",
    "    cpuUtilization = getCPUUtilizationNode(client,node)\n",
    "    podCpuUtilization = getCPUUtilizationPod(client,node,ns_name, pod_name)\n",
    "    containercpuUtilization = getCPUUtilizationPodContainer(client,node,ns_name, pod_name)\n",
    "    plt.plot(cpuUtilization.index, cpuUtilization['value'] *1000, 'r', label=\"node\") # plotting t, a separately\n",
    "    plt.plot(podCpuUtilization.index, podCpuUtilization['value'], 'b', label=\"pod\") # plotting t, b separately\n",
    "    plt.plot(containercpuUtilization.index, containercpuUtilization['value'], 'g', label=\"container\") # plotting t, c separately\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "def getMemoryUtilizationNode(client,node):\n",
    "    queryResult = client.query('SELECT * FROM \"memory/node_utilization\" where nodename = \\''+node+'\\' AND type=\\'node\\';')\n",
    "    dfmemUtilization = pd.DataFrame(queryResult['memory/node_utilization'])\n",
    "    return dfmemUtilization\n",
    "def getMemoryUtilizationPod(client,node,ns_name, pod_name):\n",
    "    queryResult = client.query('SELECT * FROM \"memory/usage\" where nodename = \\''+node+'\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+'\\'  AND type=\\'pod\\';')\n",
    "    dfmemUtilization = pd.DataFrame(queryResult['memory/usage'])\n",
    "    return dfmemUtilization\n",
    "def getMemoryUtilizationPodContainer(client,node,ns_name, pod_name):\n",
    "    queryResult = client.query('SELECT * FROM \"memory/usage\" where nodename = \\''+node+'\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+'\\' AND type=\\'pod_container\\';')\n",
    "    dfmemUtilization = pd.DataFrame(queryResult['memory/usage'])\n",
    "    return dfmemUtilization\n",
    "def prepareMemoryUtilization(client,node,ns_name, pod_name):\n",
    "    memoryUtilization = getMemoryUtilizationNode(client,node)\n",
    "    podMemoryUtilization = getMemoryUtilizationPod(client,node,ns_name, pod_name)\n",
    "    containerMemoryUtilization = getMemoryUtilizationPodContainer(client,node,ns_name, pod_name)\n",
    "    plt.plot(memoryUtilization.index, memoryUtilization['value'], 'r', label=\"node\") # plotting t, a separately\n",
    "    plt.plot(podMemoryUtilization.index, podMemoryUtilization['value'], 'b', label=\"pod\") # plotting t, b separately\n",
    "    plt.plot(containerMemoryUtilization.index, containerMemoryUtilization['value'], 'g', label=\"container\") # plotting t, c separately\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "def getNetworkTxRatePod(client,node,ns_name, pod_name):\n",
    "    queryResult = client.query('SELECT * FROM \"network/tx_rate\" where nodename = \\''+node+'\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+'\\'  AND type=\\'pod\\';')\n",
    "    dfmemUtilization = pd.DataFrame(queryResult['network/tx_rate'])\n",
    "    return dfmemUtilization\n",
    "def getNetworkTxPod(client,node,ns_name, pod_name):\n",
    "    queryResult = client.query('SELECT * FROM \"network/tx\" where nodename = \\''+node+'\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+'\\'  AND type=\\'pod\\';')\n",
    "    dfmemUtilization = pd.DataFrame(queryResult['network/tx'])\n",
    "    return dfmemUtilization\n",
    "def getNetworkTxErrorsPod(client,node,ns_name, pod_name):\n",
    "    queryResult = client.query('SELECT * FROM \"network/tx_errors\" where nodename = \\''+node+'\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+'\\'  AND type=\\'pod\\';')\n",
    "    dfmemUtilization = pd.DataFrame(queryResult['network/tx_errors'])\n",
    "    return dfmemUtilization\n",
    "def getNetworkTxErrorsRatePod(client,node,ns_name, pod_name):\n",
    "    queryResult = client.query('SELECT * FROM \"network/tx_errors_rate\" where nodename = \\''+node+'\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+'\\'  AND type=\\'pod\\';')\n",
    "    dfmemUtilization = pd.DataFrame(queryResult['network/tx_errors_rate'])\n",
    "    return dfmemUtilization\n",
    "def prepareNetworkTxRateUtilization(client,node,ns_name, pod_name):\n",
    "    podNetworTxRate = getNetworkTxRatePod(client,node,ns_name, pod_name)\n",
    "    podNetworTx = getNetworkTxPod(client,node,ns_name, pod_name)\n",
    "    podNetworkError = getNetworkTxErrorsPod(client,node,ns_name, pod_name)\n",
    "    podNetworkErrorRate = getNetworkTxErrorsRatePod(client,node,ns_name, pod_name)\n",
    "    plt.plot(podNetworTxRate.index, podNetworTxRate['value'], 'b') # plotting t, b separately\n",
    "    #plt.plot(podNetworTx.index, podNetworTx['value'], 'g') # plotting t, b separately\n",
    "    #plt.plot(podNetworkError.index, podNetworkError['value'], 'y') # plotting t, b separately\n",
    "    plt.plot(podNetworkErrorRate.index, podNetworkErrorRate['value'], 'r') # plotting t, b separately\n",
    "    plt.show()\n",
    "def getNetworkRxRatePod(client,node,ns_name, pod_name):\n",
    "    queryResult = client.query('SELECT * FROM \"network/rx_rate\" where nodename = \\''+node+'\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+'\\'  AND type=\\'pod\\';')\n",
    "    dfmemUtilization = pd.DataFrame(queryResult['network/rx_rate'])\n",
    "    return dfmemUtilization\n",
    "def getNetworkRxPod(client,node,ns_name, pod_name):\n",
    "    queryResult = client.query('SELECT * FROM \"network/rx\" where nodename = \\''+node+'\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+'\\'  AND type=\\'pod\\';')\n",
    "    dfmemUtilization = pd.DataFrame(queryResult['network/rx'])\n",
    "    return dfmemUtilization\n",
    "\n",
    "def getNetworkRxErrorsPod(client,node,ns_name, pod_name):\n",
    "    queryResult = client.query('SELECT * FROM \"network/rx_errors\" where nodename = \\''+node+'\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+'\\'  AND type=\\'pod\\';')\n",
    "    dfmemUtilization = pd.DataFrame(queryResult['network/rx_errors'])\n",
    "    return dfmemUtilization\n",
    "def getNetworkRxErrorsRatePod(client,node,ns_name, pod_name):\n",
    "    queryResult = client.query('SELECT * FROM \"network/rx_errors_rate\" where nodename = \\''+node+'\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+'\\'  AND type=\\'pod\\';')\n",
    "    dfmemUtilization = pd.DataFrame(queryResult['network/rx_errors_rate'])\n",
    "    return dfmemUtilization\n",
    "def prepareNetworkRxRateUtilization(client,node,ns_name, pod_name):\n",
    "    podNetworRxRate = getNetworkRxRatePod(client,node,ns_name, pod_name)\n",
    "    podNetworRx = getNetworkRxPod(client,node,ns_name, pod_name)\n",
    "    podNetworkError = getNetworkRxErrorsPod(client,node,ns_name, pod_name)\n",
    "    podNetworkErrorRate = getNetworkRxErrorsRatePod(client,node,ns_name, pod_name)\n",
    "    plt.plot(podNetworRxRate.index, podNetworRxRate['value'], 'b') # plotting t, b separately\n",
    "    #plt.plot(podNetworRx.index, podNetworRx['value'], 'g') # plotting t, b separately\n",
    "    #plt.plot(podNetworkError.index, podNetworkError['value'], 'y') # plotting t, b separately\n",
    "    plt.plot(podNetworkErrorRate.index, podNetworkErrorRate['value'], 'r') # plotting t, b separately\n",
    "    plt.show()\n",
    "def getRelevantNodeName(client,ns_name):\n",
    "    allNodeNames  = getAllNodeNames(client)\n",
    "    #nsNames = getNamespaceNames(allNodeNames[0])\n",
    "    for node in allNodeNames:\n",
    "        allPodNamesNode = getAllPodNames(client,node,ns_name)\n",
    "        if(not allPodNamesNode.empty):\n",
    "            return node\n",
    "\n",
    "def getNodeResourceUtilizationDf(client, nodeName):\n",
    "    Result_node_CPU = client.query(\"SELECT value from \\\"cpu/node_utilization\\\" where nodename = '\"+nodeName+\"' AND type = 'node' \")\n",
    "    Result_node_MEM = client.query(\"SELECT value from \\\"memory/node_utilization\\\" where nodename = '\"+nodeName+\"' AND type = 'node' \")\n",
    "\n",
    "    Result_node_CPU_Cores = client.query(\"SELECT mean(\\\"value\\\") FROM \\\"cpu/node_capacity\\\" where nodename = '\"+nodeName+\n",
    "                                         \"' AND type = 'node' GROUP BY time(1m)\")\n",
    "    Result_node_mem_node = client.query(\"SELECT mean(\\\"value\\\")FROM \\\"memory/node_capacity\\\" where nodename = '\"+\n",
    "                                        nodeName+\"' AND type = 'node' GROUP BY time(1m)\")\n",
    "\n",
    "    cpu_points = pd.DataFrame(Result_node_CPU.get_points())\n",
    "    cpu_points['time'] = pd.to_datetime(cpu_points['time'])\n",
    "    cpu_points = cpu_points.set_index('time')\n",
    "    cpu_points.columns = ['node_cpu_util']\n",
    "    mem_points = pd.DataFrame(Result_node_MEM.get_points())\n",
    "    mem_points['time'] = pd.to_datetime(mem_points['time'])\n",
    "    mem_points = mem_points.set_index('time')\n",
    "    mem_points.columns = ['node_mem_util']\n",
    "\n",
    "    cores_points = pd.DataFrame(Result_node_CPU_Cores.get_points())\n",
    "    cores_points['time'] = pd.to_datetime(cores_points['time'])\n",
    "    cores_points = cores_points.set_index('time')\n",
    "    cores_points.columns = ['node_cores']\n",
    "\n",
    "    mem_node_points = pd.DataFrame(Result_node_mem_node.get_points())\n",
    "    mem_node_points['time'] = pd.to_datetime(mem_node_points['time'])\n",
    "    mem_node_points = mem_node_points.set_index('time')\n",
    "    mem_node_points.columns = ['node_mem']\n",
    "\n",
    "    df_node =pd.concat([cpu_points, mem_points,cores_points,mem_node_points], axis=1)\n",
    "    return df_node\n",
    "def getPodResourceUtilizationDf(client, node, ns_name, pod_name):\n",
    "    Result_Pod_CPU_usage = client.query('SELECT value FROM \"cpu/usage_rate\" where nodename = \\''+node+\n",
    "                                        '\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+\n",
    "                                        '\\'  AND type=\\'pod\\';')\n",
    "    Result_Pod_MEM_usage = client.query('SELECT value from \\\"memory/usage\\\" where nodename = \\''+node+\n",
    "                                        '\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+\n",
    "                                        '\\'  AND type=\\'pod\\';')\n",
    "\n",
    "    Result_Pod_CPU_limit = client.query('SELECT mean(\\\"value\\\") FROM \"cpu/limit\" where nodename = \\''+node+'\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+'\\'  AND type=\\'pod\\' group by time(1m);')\n",
    "    Result_Pod_MEM_limit = client.query('SELECT mean(\\\"value\\\") from \\\"memory/limit\\\" where nodename = \\''+node+\n",
    "                                        '\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+\n",
    "                                        '\\'  AND type=\\'pod\\' group by time(1m);')\n",
    "\n",
    "    Result_Pod_CPU_requests = client.query('SELECT mean(\\\"value\\\") FROM \"cpu/request\" where nodename = \\''+node+'\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+'\\'  AND type=\\'pod\\' group by time(1m);')\n",
    "    Result_Pod_MEM_requests = client.query('SELECT mean(\\\"value\\\") from \\\"memory/request\\\" where nodename = \\''+node+\n",
    "                                           '\\' AND pod_name = \\''+pod_name+'\\' AND namespace_name = \\''+ns_name+\n",
    "                                           '\\'  AND type=\\'pod\\' group by time(1m);')\n",
    "\n",
    "\n",
    "    cpu_points_usage = pd.DataFrame(Result_Pod_CPU_usage.get_points())\n",
    "    cpu_points_usage['time'] = pd.to_datetime(cpu_points_usage['time'])\n",
    "    cpu_points_usage = cpu_points_usage.set_index('time')\n",
    "    cpu_points_usage.columns = ['pod_cpu_usage']\n",
    "\n",
    "\n",
    "    mem_points_usage = pd.DataFrame(Result_Pod_MEM_usage.get_points())\n",
    "    mem_points_usage['time'] = pd.to_datetime(mem_points_usage['time'])\n",
    "    mem_points_usage = mem_points_usage.set_index('time')\n",
    "    mem_points_usage.columns = ['pod_mem_usage']\n",
    "\n",
    "\n",
    "    cpu_points_limits = pd.DataFrame(Result_Pod_CPU_limit.get_points())\n",
    "    cpu_points_limits['time'] = pd.to_datetime(cpu_points_limits['time'])\n",
    "    cpu_points_limits = cpu_points_limits.set_index('time')\n",
    "    cpu_points_limits.columns = ['pod_cpu_limit']\n",
    "\n",
    "\n",
    "    mem_points_limits = pd.DataFrame(Result_Pod_MEM_limit.get_points())\n",
    "    mem_points_limits['time'] = pd.to_datetime(mem_points_limits['time'])\n",
    "    mem_points_limits = mem_points_limits.set_index('time')\n",
    "    mem_points_limits.columns = ['pod_mem_limit']\n",
    "\n",
    "\n",
    "    cpu_points_request = pd.DataFrame(Result_Pod_CPU_requests.get_points())\n",
    "    cpu_points_request['time'] = pd.to_datetime(cpu_points_request['time'])\n",
    "    cpu_points_request = cpu_points_request.set_index('time')\n",
    "    cpu_points_request.columns = ['pod_cpu_request']\n",
    "\n",
    "\n",
    "    mem_points_request = pd.DataFrame(Result_Pod_MEM_requests.get_points())\n",
    "    mem_points_request['time'] = pd.to_datetime(mem_points_request['time'])\n",
    "    mem_points_request = mem_points_request.set_index('time')\n",
    "    mem_points_request.columns = ['pod_mem_request']\n",
    "\n",
    "    df_pod =pd.concat([cpu_points_usage, mem_points_usage,cpu_points_limits,mem_points_limits,cpu_points_request,mem_points_request ], axis=1)\n",
    "\n",
    "    return df_pod\n",
    "def getRequestsDf(clientK6):\n",
    "    queryResult = clientK6.query('SELECT sum(\"value\") FROM \"vus\" group by time(1m);')\n",
    "    vus = pd.DataFrame(queryResult['vus'])\n",
    "    vus.columns = ['vus','time']\n",
    "    vus = vus.set_index('time')\n",
    "\n",
    "\n",
    "    queryResultReqs = clientK6.query('SELECT sum(\"value\") FROM \"http_reqs\" group by time(1m);')\n",
    "    reqs = pd.DataFrame(queryResultReqs['http_reqs'])\n",
    "    reqs.columns = ['requests','time']\n",
    "    reqs = reqs.set_index('time')\n",
    "    queryResultReqsDuration95 = clientK6.query('SELECT percentile(\"value\", 95) FROM \"http_req_duration\" group by time(1m) ;')\n",
    "    reqs_duration95 = pd.DataFrame(queryResultReqsDuration95['http_req_duration'])\n",
    "    reqs_duration95.columns = [ 'requests_duration_percentile_95','time']\n",
    "    reqs_duration95 = reqs_duration95.set_index('time')\n",
    "    queryResultReqsDuration90 = clientK6.query('SELECT percentile(\"value\", 90) FROM \"http_req_duration\" group by time(1m) ;')\n",
    "    reqs_duration90 = pd.DataFrame(queryResultReqsDuration90['http_req_duration'])\n",
    "    reqs_duration90.columns = ['requests_duration_percentile_90','time']\n",
    "    reqs_duration90 = reqs_duration90.set_index('time')\n",
    "\n",
    "    queryResultMaxDuration = clientK6.query('SELECT max(\"value\") FROM \"http_req_duration\" group by time(1m);')\n",
    "    reqs_duration_max = pd.DataFrame(queryResultMaxDuration['http_req_duration'])\n",
    "    reqs_duration_max.columns = ['requests_duration_max','time']\n",
    "    reqs_duration_max = reqs_duration_max.set_index('time')\n",
    "\n",
    "    queryResultMinDuration = clientK6.query('SELECT min(\"value\") FROM \"http_req_duration\" group by time(1m);')\n",
    "    reqs_duration_min = pd.DataFrame(queryResultMinDuration['http_req_duration'])\n",
    "    reqs_duration_min.columns = ['requests_duration_min','time']\n",
    "    reqs_duration_min = reqs_duration_min.set_index('time')\n",
    "\n",
    "    queryResultMeanDuration = clientK6.query('SELECT mean(\"value\") FROM \"http_req_duration\" group by time(1m);')\n",
    "    reqs_duration_mean = pd.DataFrame(queryResultMeanDuration['http_req_duration'])\n",
    "    reqs_duration_mean.columns = ['requests_duration_mean','time']\n",
    "    reqs_duration_mean = reqs_duration_mean.set_index('time')\n",
    "\n",
    "    queryResultMedianDuration = clientK6.query('SELECT median(\"value\") FROM \"http_req_duration\" group by time(1m);')\n",
    "    reqs_duration_median = pd.DataFrame(queryResultMedianDuration['http_req_duration'])\n",
    "    reqs_duration_median.columns = ['requests_duration_median','time']\n",
    "    reqs_duration_median = reqs_duration_median.set_index('time')\n",
    "\n",
    "    finalDF = pd.merge(vus, reqs, left_index=True, right_index=True)\n",
    "    finalDF = pd.merge(finalDF, reqs_duration95, left_index=True, right_index=True)\n",
    "    finalDF = pd.merge(finalDF, reqs_duration90, left_index=True, right_index=True)\n",
    "    finalDF = pd.merge(finalDF,reqs_duration_max, left_index=True, right_index=True)\n",
    "    finalDF = pd.merge(finalDF,reqs_duration_min, left_index=True, right_index=True)\n",
    "    finalDF = pd.merge(finalDF,reqs_duration_mean, left_index=True, right_index=True)\n",
    "    finalDF = pd.merge(finalDF,reqs_duration_median, left_index=True, right_index=True)\n",
    "    finalDF.index = pd.to_datetime(finalDF.index)\n",
    "\n",
    "    return finalDF\n",
    "\n",
    "def getPodsNodesRequestsDf(appNames, client,  clientK6):\n",
    "    default_ns_name =  \"default\"\n",
    "    relevantNodeName = getRelevantNodeName(client,default_ns_name)\n",
    "    df_pods_node = []\n",
    "    if relevantNodeName is not None:\n",
    "        podNames = getAllPodNames(client,relevantNodeName, default_ns_name)\n",
    "        df_node = getNodeResourceUtilizationDf(client,relevantNodeName)\n",
    "\n",
    "        for podName in podNames:\n",
    "            if appNames[0] in podName:\n",
    "                df_pod = getPodResourceUtilizationDf(client,relevantNodeName, default_ns_name, podName)\n",
    "                finalDF = pd.merge(df_node,df_pod, left_index=True, right_index=True)\n",
    "                requestsDF = getRequestsDf(clientK6)\n",
    "                finalDF = pd.merge(finalDF,requestsDF, left_index=True, right_index=True)\n",
    "                if(finalDF['pod_cpu_limit'].values[0]==0):\n",
    "                    finalDF['pod_cpu_usage'] = finalDF['pod_cpu_usage']/(finalDF['node_cores'])\n",
    "                    finalDF['pod_cpu_limit'] = finalDF['node_cores']/1000\n",
    "                    finalDF['pod_cpu_request'] = finalDF['node_cores']/1000\n",
    "                else:\n",
    "                    finalDF['pod_cpu_usage'] = finalDF['pod_cpu_usage']/(finalDF['pod_cpu_limit'])\n",
    "                    finalDF['pod_cpu_limit'] = finalDF['pod_cpu_limit']/1000\n",
    "                    finalDF['pod_cpu_request'] = finalDF['pod_cpu_request']/1000\n",
    "\n",
    "                if(finalDF['pod_mem_limit'].values[0]==0):\n",
    "                    finalDF['pod_mem_usage'] = finalDF['pod_mem_usage']/(finalDF['node_mem'])\n",
    "                    finalDF['pod_mem_limit'] = finalDF['node_mem']/(1073741824)\n",
    "                    finalDF['pod_mem_request'] = finalDF['node_mem']/(1073741824)\n",
    "                else:\n",
    "                    finalDF['pod_mem_usage'] = finalDF['pod_mem_usage']/(finalDF['pod_mem_limit'])\n",
    "                    finalDF['pod_mem_limit'] = finalDF['pod_mem_limit']/(1073741824)\n",
    "                    finalDF['pod_mem_request'] = finalDF['pod_mem_request']/(1073741824)\n",
    "                finalDF['node_cores'] = finalDF['node_cores']/1000\n",
    "                finalDF['node_mem'] = finalDF['node_mem']/(1073741824)\n",
    "\n",
    "                finalDF = finalDF.fillna(0)\n",
    "                finalDF = finalDF[(finalDF.T != 0).any()]\n",
    "                df_pods_node.append(finalDF)\n",
    "    return df_pods_node\n",
    "\n",
    "def getAndCombineAllDbs( host, port, username, password,appNames, folderNames):\n",
    "    allFinalDFs = []\n",
    "    print(\"FolderNames len = \", len(folderNames))\n",
    "    for folderName in folderNames:\n",
    "        client = InfluxDBClient(host, port,username , password, folderName+'_k8s')\n",
    "        clientK6 = InfluxDBClient(host, port, username, password, folderName+'_TestK6')\n",
    "        df_pods_node = getPodsNodesRequestsDf(appNames, client, clientK6)\n",
    "        print(folderName)\n",
    "        if(len(df_pods_node)>0):\n",
    "            finalDF = pd.DataFrame()\n",
    "            finalDF['pod_util_cpu_sum'] = 0\n",
    "            finalDF['pod_util_mem_sum'] = 0\n",
    "            first = 1\n",
    "            for i in range(len(df_pods_node)):\n",
    "                df_pods_node[i] = df_pods_node[i].reset_index(drop=True)\n",
    "                if(first==1):\n",
    "                    finalDF['pod_util_cpu_sum'] = df_pods_node[i]['pod_cpu_usage']\n",
    "                    finalDF['pod_util_mem_sum'] = df_pods_node[i]['pod_mem_usage']\n",
    "                    first=0\n",
    "                else:\n",
    "                    finalDF['pod_util_cpu_sum'] = finalDF['pod_util_cpu_sum'] +  df_pods_node[i]['pod_cpu_usage']\n",
    "                    finalDF['pod_util_mem_sum'] = finalDF['pod_util_mem_sum'] +  df_pods_node[i]['pod_mem_usage']\n",
    "\n",
    "            finalDF['num_pods'] = int(len(df_pods_node))\n",
    "            finalDF['pod_util_cpu_avg'] = finalDF['pod_util_cpu_sum']/finalDF['num_pods']\n",
    "            finalDF['pod_util_mem_avg'] = finalDF['pod_util_mem_sum']/finalDF['num_pods']\n",
    "\n",
    "            finalDF = pd.concat([finalDF, df_pods_node[0][['node_cores', 'node_mem','node_cpu_util','node_mem_util', 'pod_cpu_limit', 'pod_cpu_request','pod_mem_limit',\n",
    "                                                           'pod_mem_request','vus','requests','requests_duration_percentile_95',\n",
    "                                                           'requests_duration_percentile_90','requests_duration_max', 'requests_duration_min',\n",
    "                                                           'requests_duration_mean', 'requests_duration_median'\n",
    "                                                           ]]], axis=1)\n",
    "\n",
    "            allFinalDFs.append(finalDF)\n",
    "    df = pd.DataFrame()\n",
    "    print(\"All Dfs len = \", len(allFinalDFs))\n",
    "    for idx in range(len(allFinalDFs)):\n",
    "        df = df.append(allFinalDFs[idx])\n",
    "\n",
    "    final_df  = df[['requests','requests_duration_mean','num_pods','pod_cpu_limit','node_cores', 'node_mem','pod_mem_limit','pod_util_cpu_avg','pod_util_mem_avg',\n",
    "                    ]]\n",
    "    final_df['pod_util_cpu_avg'] = final_df['pod_util_cpu_avg']*final_df['pod_cpu_limit']\n",
    "    final_df['pod_util_mem_avg'] = final_df['pod_util_mem_avg']*final_df['pod_mem_limit']\n",
    "    final_df = final_df.sort_values(['requests'])\n",
    "    final_df = final_df[(final_df[['pod_util_cpu_avg','pod_util_mem_avg','requests_duration_mean']] != 0).all(axis=1)]\n",
    "    final_df = final_df[np.isfinite(final_df['requests'])]\n",
    "    final_df = final_df[np.isfinite(final_df['requests_duration_mean'])]\n",
    "    final_df = final_df[np.isfinite(final_df['pod_util_cpu_avg'])]\n",
    "    final_df = final_df[np.isfinite(final_df['pod_util_mem_avg'])]\n",
    "    final_df = final_df[final_df.requests_duration_mean < 2500]\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "    return final_df\n",
    "\n",
    "def train_and_return_model(host, port, username, password,appType, appNames, folderNames ):\n",
    "    df = getAndCombineAllDbs(host, port, username, password,appNames, folderNames)\n",
    "    df = df.sort_values(['pod_util_cpu_avg'])\n",
    "    df_X = df[['pod_util_cpu_avg','pod_cpu_limit','pod_mem_limit', 'num_pods', 'requests_duration_mean']].values\n",
    "    df_Y = df[['requests']].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size=0.33, random_state=42)\n",
    "    polybig_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = linear_model.LinearRegression()\n",
    "    regr = Pipeline([\n",
    "        (\"poly_features\", polybig_features),\n",
    "        (\"std_scaler\", std_scaler),\n",
    "        (\"lin_reg\", lin_reg),\n",
    "    ])\n",
    "    regr.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = regr.predict(X_test)\n",
    "\n",
    "    # The coefficients\n",
    "\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "          % mean_squared_error(y_test, y_pred))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "    #print ('Test score %.2f', regr.score(X_test, y_test) )\n",
    "    print(\"Train Mean squared error: %.2f\"\n",
    "          % mean_squared_error(y_train, regr.predict(X_train)))\n",
    "    rms = sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print('RMs score: %.2f' % rms)\n",
    "    return regr, rms\n",
    "def train_and_return_model_replicas(host, port, username, password,appType, appNames, folderNames ):\n",
    "    df = getAndCombineAllDbs(host, port, username, password,appNames, folderNames)\n",
    "    df = df.sort_values(['pod_util_cpu_avg'])\n",
    "    df_X = df[['pod_util_cpu_avg','pod_cpu_limit','pod_mem_limit', 'requests', 'requests_duration_mean']].values\n",
    "    df_Y = df[['num_pods']].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size=0.33, random_state=42)\n",
    "    polybig_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = linear_model.LinearRegression()\n",
    "    regr = Pipeline([\n",
    "        (\"poly_features\", polybig_features),\n",
    "        (\"std_scaler\", std_scaler),\n",
    "        (\"lin_reg\", lin_reg),\n",
    "    ])\n",
    "    regr.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = regr.predict(X_test)\n",
    "\n",
    "    # The coefficients\n",
    "\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "          % mean_squared_error(y_test, y_pred))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "    #print ('Test score %.2f', regr.score(X_test, y_test) )\n",
    "    print(\"Train Mean squared error: %.2f\"\n",
    "          % mean_squared_error(y_train, regr.predict(X_train)))\n",
    "    rms = sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print('RMs score: %.2f' % rms)\n",
    "    return regr, rms\n",
    "\n",
    "def train_and_return_model_smart(host, port, username, password,appType, appNames, folderNames ):\n",
    "    df = getAndCombineAllDbs(host, port, username, password,appNames, folderNames)\n",
    "    df = df.sort_values(['pod_util_cpu_avg'])\n",
    "    df = df.head(50)\n",
    "    if(appType == \"compute\"):\n",
    "        df_X = df[['pod_util_cpu_avg','requests_duration_mean']].values\n",
    "    else:\n",
    "        df_X = df[['pod_util_cpu_avg','requests_duration_mean']].values\n",
    "\n",
    "    df_Y = df[['requests']].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size=0.33, random_state=42)\n",
    "    polybig_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = linear_model.LinearRegression()\n",
    "    regr = Pipeline([\n",
    "        (\"poly_features\", polybig_features),\n",
    "        (\"std_scaler\", std_scaler),\n",
    "        (\"lin_reg\", lin_reg),\n",
    "    ])\n",
    "    regr.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = regr.predict(X_test)\n",
    "\n",
    "    # The coefficients\n",
    "\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "          % mean_squared_error(y_test, y_pred))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "    #print ('Test score %.2f', regr.score(X_test, y_test) )\n",
    "    print(\"Train Mean squared error: %.2f\"\n",
    "          % mean_squared_error(y_train, regr.predict(X_train)))\n",
    "    rms = sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print('RMs score: %.2f' % rms)\n",
    "    return regr, rms\n",
    "def do_training( host, port, username, password, instanceFamily, appNames, appType, folderNames, filename):\n",
    "    model, rms = train_and_return_model(host, port, username, password,appType, appNames,folderNames)\n",
    "    print(filename)\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    return rms\n",
    "def do_training_replicas( host, port, username, password, instanceFamily, appNames, appType, folderNames, filename):\n",
    "    model, rms = train_and_return_model_replicas(host, port, username, password,appType, appNames,folderNames)\n",
    "    print(filename)\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    return rms\n",
    "\n",
    "def do_training_smart( host, port, username, password, instanceFamily, appNames, appType, folderNames, filename):\n",
    "    model, rms = train_and_return_model_smart(host, port, username, password,appType, appNames,folderNames)\n",
    "    print(filename)\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    return rms\n",
    "def get_folder_names( appName, colName, dbName):\n",
    "    mongoclient = MongoClient(mongo_host, 27017, username=mongo_username, password=mongo_password)\n",
    "    db = mongoclient[dbName]\n",
    "    col = db[colName]\n",
    "    datapoints = list(col.find({\"servicename\": appName}))\n",
    "    dfMongo = json_normalize(datapoints)\n",
    "    return dfMongo.foldername\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "appNamesConst = ['primeapp', 'movieapp', 'webacapp', 'serveapp', 'mixalapp']\n",
    "appTypesConst = ['compute', 'dbaccess', 'web', 'sandbox', 'mix']\n",
    "mainServiceNamesConst =[[0 for x in range(5)] for y in range(5)]\n",
    "mainServiceNamesConst[0] = ['primeapp']\n",
    "mainServiceNamesConst[1] = ['movieapp']\n",
    "mainServiceNamesConst[2] = ['webacapp']\n",
    "mainServiceNamesConst[3] = ['serveapp']\n",
    "mainServiceNamesConst[4] = ['primeapp', 'movieapp', 'webacapp', 'serveapp']\n",
    "appTest = 0\n",
    "appName = appNamesConst[appTest]\n",
    "appType = appTypesConst[appTest]\n",
    "instanceFamily = \"t2\"\n",
    "mainServiceName = mainServiceNamesConst[appTest][0]\n",
    "requestDuration = 1000\n",
    "\n",
    "host = '141.40.254.24'\n",
    "port = '8086'\n",
    "username= 'root'\n",
    "password = 'root'\n",
    "\n",
    "mongo_host = '141.40.254.24'\n",
    "mongo_port = 27017\n",
    "mongo_username= 'mongoDBSecureUser12378'\n",
    "mongo_password = 'youdonthavetoknowthis8998'\n",
    "\n",
    "colName = \"ALL_BRUTE_FORCE_CONDUCTED_TEST_NAMES\"\n",
    "dbName = \"TERMINUS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primeapp ,  primeapp ,  compute\n"
     ]
    }
   ],
   "source": [
    "print (appName, \", \", mainServiceName, \", \", appType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ServerSelectionTimeoutError",
     "evalue": "141.40.254.24:27017: timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mServerSelectionTimeoutError\u001b[0m               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3bf30a9434f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfolderNamesPrimeapp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_folder_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappNamesConst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfolderNamesMovieapp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_folder_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappNamesConst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfolderNamesWebapp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_folder_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappNamesConst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfolderNamesMixapp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_folder_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappNamesConst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-1f804d0b84b1>\u001b[0m in \u001b[0;36mget_folder_names\u001b[1;34m(appName, colName, dbName)\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmongoclient\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdbName\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolName\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m     \u001b[0mdatapoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"servicename\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mappName\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m     \u001b[0mdfMongo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatapoints\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdfMongo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfoldername\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\pymongo\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1130\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__empty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1132\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__data\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_refresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1133\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__manipulate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m                 \u001b[0m_db\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__collection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\pymongo\\cursor.py\u001b[0m in \u001b[0;36m_refresh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1053\u001b[0m                                        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m                                        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__read_concern\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1055\u001b[1;33m                                        self.__collation))\n\u001b[0m\u001b[0;32m   1056\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__killed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\pymongo\\cursor.py\u001b[0m in \u001b[0;36m__send_message\u001b[1;34m(self, operation)\u001b[0m\n\u001b[0;32m    890\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m                 response = client._send_message_with_response(operation,\n\u001b[1;32m--> 892\u001b[1;33m                                                               **kwargs)\n\u001b[0m\u001b[0;32m    893\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__address\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__exhaust\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\pymongo\\mongo_client.py\u001b[0m in \u001b[0;36m_send_message_with_response\u001b[1;34m(self, operation, read_preference, exhaust, address)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_preference\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mwritable_server_selector\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m             \u001b[0mserver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_server\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;31m# A _Query's slaveOk bit is already set for queries with non-primary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\pymongo\\topology.py\u001b[0m in \u001b[0;36mselect_server\u001b[1;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[0;32m    212\u001b[0m         return random.choice(self.select_servers(selector,\n\u001b[0;32m    213\u001b[0m                                                  \u001b[0mserver_selection_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m                                                  address))\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     def select_server_by_address(self, address,\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\pymongo\\topology.py\u001b[0m in \u001b[0;36mselect_servers\u001b[1;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mserver_timeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnow\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                     raise ServerSelectionTimeoutError(\n\u001b[1;32m--> 189\u001b[1;33m                         self._error_message(selector))\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_opened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mServerSelectionTimeoutError\u001b[0m: 141.40.254.24:27017: timed out"
     ]
    }
   ],
   "source": [
    "folderNamesPrimeapp = get_folder_names(appNamesConst[0], colName, dbName)\n",
    "folderNamesMovieapp = get_folder_names(appNamesConst[1], colName, dbName)\n",
    "folderNamesWebapp = get_folder_names(appNamesConst[2], colName, dbName)\n",
    "folderNamesMixapp = get_folder_names(appNamesConst[3], colName, dbName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderNames = folderNamesPrimeapp\n",
    "appNames = [mainServiceName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfApp = getAndCombineAllDbs(host, port, username, password,appNames, folderNames)\n",
    "appName = appName+ \"_\"+ mainServiceName + \".csv\"\n",
    "dfApp.to_csv(appName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FolderNames len =  61\n",
      "sv1t1rc1nc1t2microdbaccessmovieappfalseloya\n",
      "sv1t1rc1nc1t2nanodbaccessmovieappfalsezoss\n",
      "sv1t1rc1nc1t2smalldbaccessmovieappfalsec2tp\n",
      "s1t1rc1nc1t2mediumdbaccessmovieappt2microy64\n",
      "s1t1rc1nc1t2mediumdbaccessmovieappt2nanomtp\n",
      "s1t1rc1nc1t2mediumdbaccessmovieappt2small3rz\n",
      "sv1t1rc1nc1t2mediumdbaccessmovieappfalsedv3acs\n",
      "s1t1rc1nc1t2largedbaccessmovieappt2micro8uf\n",
      "s1t1rc1nc1t2largedbaccessmovieappt2nanofch\n",
      "s1t1rc1nc1t2largedbaccessmovieappt2smallhs9\n",
      "s1t1rc1nc1t2xlargedbaccessmovieappt2smallpn8\n",
      "sv1t1rc1nc1t2largedbaccessmovieappfalsee1hics\n",
      "s1t1rc1nc1t2xlargedbaccessmovieappt2nanor1n\n",
      "s1t1rc1nc1t2xlargedbaccessmovieappt2mediumdvw\n",
      "s1t1rc1nc1t2xlargedbaccessmovieappt2largejn8\n",
      "s1t1rc1nc1t2xlargedbaccessmovieappt2micro8zu\n",
      "s1t1rc2nc1t2xlargedbaccessmovieapp500r9z\n",
      "s1t1rc1nc1t2xlargedbaccessmovieappt2mediumq4q\n",
      "s1t1rc3nc1t2xlargedbaccessmovieapp100k2g\n",
      "s1t1rc3nc1t2xlargedbaccessmovieapp500hjl\n",
      "s1t1rc1nc1t2xlargedbaccessmovieapp5003ba\n",
      "s1t1rc1nc1t2xlargedbaccessmovieappt2smallosm\n",
      "s1t1rc3nc1t2xlargedbaccessmovieapp200cz4\n",
      "s1t1rc2nc1t2xlargedbaccessmovieappt2nanobpl\n",
      "s1t1rc1nc1t2mediumdbaccessmovieapp200k9x\n",
      "s1t1rc1nc1t2mediumdbaccessmovieapp1009jo\n",
      "s1t1rc1nc1t2largedbaccessmovieappt2micrornm\n",
      "s1t1rc1nc1t2largedbaccessmovieapp100lxi\n",
      "s1t1rc1nc1t2largedbaccessmovieapp200qp6\n",
      "s1t1rc1nc1t2largedbaccessmovieapp500p23\n",
      "s1t1rc2nc1t2largedbaccessmovieapp1008yw\n",
      "s1t1rc3nc1t2largedbaccessmovieapp100zf9\n",
      "s1t1rc3nc1t2largedbaccessmovieapp200ygf\n",
      "s1t1rc3nc1t2largedbaccessmovieapp500r4q\n",
      "s1t1rc1nc1t2nanodbaccessmovieapp100tp5\n",
      "s1t1rc1nc1t2nanodbaccessmovieapp200pdr\n",
      "s1t1rc2nc1t2nanodbaccessmovieapp1003ao\n",
      "s1t1rc1nc2t2nanodbaccessmovieapp500rc6\n",
      "s1t1rc2nc1t2nanodbaccessmovieapp2006rg\n",
      "s1t1rc3nc2t2nanodbaccessmovieapp200hfo\n",
      "s1t1rc2nc2t2nanodbaccessmovieapp200z5d\n",
      "s1t1rc2nc3t2nanodbaccessmovieapp500gtb\n",
      "s1t1rc1nc1t2microdbaccessmovieapp100jj9\n",
      "s1t1rc2nc1t2microdbaccessmovieapp100z4o\n",
      "s1t1rc3nc1t2nanodbaccessmovieapp100hxe\n",
      "s1t1rc3nc4t2nanodbaccessmovieapp500iic\n",
      "s1t1rc3nc1t2microdbaccessmovieapp100oo9\n",
      "s1t1rc1nc1t2microdbaccessmovieapp200pvj\n",
      "s1t1rc2nc1t2microdbaccessmovieapp200rqg\n",
      "s1t1rc3nc1t2microdbaccessmovieapp200exg\n",
      "s1t1rc1nc1t2smalldbaccessmovieapp100g57\n",
      "s1t1rc2nc1t2smalldbaccessmovieapp100frn\n",
      "s1t1rc3nc1t2smalldbaccessmovieapp1006yn\n",
      "s1t1rc1nc1t2smalldbaccessmovieapp200do8\n",
      "s1t1rc1nc1t2mediumdbaccessmovieapp100326\n",
      "s1t1rc3nc1t2smalldbaccessmovieapp200flx\n",
      "s1t1rc2nc1t2smalldbaccessmovieapp200y1p\n",
      "s1t1rc2nc1t2mediumdbaccessmovieapp1005y6\n",
      "s1t1rc1nc1t2largedbaccessmovieapp5009a6\n",
      "s1t1rc1nc1t2mediumdbaccessmovieapp200m2g\n",
      "s1t1rc2nc1t2mediumdbaccessmovieapp200fge\n",
      "All Dfs len =  56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ansjin\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:343: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\ansjin\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:344: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-9f6aad4a98a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdfAppMovie\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAndCombineAllDbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musername\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mappNames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfolderNames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mappNameMovie\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mappNamesConst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m\"_\"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mmainServiceNamesConst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mappNameMovie\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappNameMovie\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "folderNames = folderNamesMovieapp\n",
    "appNames = [mainServiceNamesConst[1][0]]\n",
    "dfAppMovie = getAndCombineAllDbs(host, port, username, password,appNames, folderNames)\n",
    "appNameMovie = appNamesConst[1]+ \"_\"+ mainServiceNamesConst[1][0] + \".csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAppMovie.to_csv(appNameMovie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "appName = appName+ \"_\"+ mainServiceName + \".csv\"\n",
    "df = pd.read_csv(appName, header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>requests</th>\n",
       "      <th>requests_duration_mean</th>\n",
       "      <th>num_pods</th>\n",
       "      <th>pod_cpu_limit</th>\n",
       "      <th>node_cores</th>\n",
       "      <th>node_mem</th>\n",
       "      <th>pod_mem_limit</th>\n",
       "      <th>pod_util_cpu_avg</th>\n",
       "      <th>pod_util_mem_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.0</td>\n",
       "      <td>128.577375</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.674213</td>\n",
       "      <td>0.488281</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.080709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.0</td>\n",
       "      <td>16.816887</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.955933</td>\n",
       "      <td>1.955933</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.026791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.0</td>\n",
       "      <td>16.453179</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.674213</td>\n",
       "      <td>0.976562</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.019331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.0</td>\n",
       "      <td>15.507512</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.674213</td>\n",
       "      <td>0.976562</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.021217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.0</td>\n",
       "      <td>18.539949</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.799198</td>\n",
       "      <td>0.488281</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.018446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   requests  requests_duration_mean  num_pods  pod_cpu_limit  node_cores  \\\n",
       "0      13.0              128.577375         2            0.5         4.0   \n",
       "1      44.0               16.816887         1            1.0         1.0   \n",
       "2      45.0               16.453179         2            1.0         4.0   \n",
       "3      45.0               15.507512         1            1.0         4.0   \n",
       "4      47.0               18.539949         2            0.5         2.0   \n",
       "\n",
       "    node_mem  pod_mem_limit  pod_util_cpu_avg  pod_util_mem_avg  \n",
       "0  15.674213       0.488281            0.4500          0.080709  \n",
       "1   1.955933       1.955933            0.0030          0.026791  \n",
       "2  15.674213       0.976562            0.0015          0.019331  \n",
       "3  15.674213       0.976562            0.0040          0.021217  \n",
       "4   7.799198       0.488281            0.0010          0.018446  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brute-force-training\n",
    "\n",
    "appNames = [mainServiceName]\n",
    "df = df.sort_values(['pod_util_cpu_avg'])\n",
    "df_X = df[['pod_util_cpu_avg','pod_cpu_limit','pod_mem_limit', 'num_pods', 'requests_duration_mean']].values\n",
    "df_Y = df[['requests']].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size=0.33, random_state=42)\n",
    "polybig_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "std_scaler = StandardScaler()\n",
    "lin_reg = linear_model.LinearRegression()\n",
    "regr = Pipeline([\n",
    "    (\"poly_features\", polybig_features),\n",
    "    (\"std_scaler\", std_scaler),\n",
    "    (\"lin_reg\", lin_reg),\n",
    "])\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "#print ('Test score %.2f', regr.score(X_test, y_test) )\n",
    "print(\"Train Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, regr.predict(X_train)))\n",
    "rms = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('RMs score: %.2f' % rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "appNames = [mainServiceName]\n",
    "df = df.sort_values(['pod_util_cpu_avg'])\n",
    "df_X = df[['pod_util_cpu_avg','pod_cpu_limit','pod_mem_limit', 'num_pods']].values\n",
    "df_Y = df[['requests']].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size=0.33, random_state=42)\n",
    "\n",
    "svr_rbf = SVR(kernel= 'rbf', C= 1e3, gamma= 0.1) # defining the support vector regression models\n",
    "svr_lin = SVR(kernel= 'linear', C= 1e3)\n",
    "svr_poly = SVR(kernel= 'poly', C= 1e3, degree= 2)\n",
    "svr_rbf.fit(X_train, y_train) # fitting the data points in the models\n",
    "svr_lin.fit(X_train, y_train)\n",
    "svr_poly.fit(X_train, y_train)\n",
    "\n",
    "y_predsvr_rbf = svr_rbf.predict(X_test)\n",
    "y_predsvr_lin = svr_lin.predict(X_test)\n",
    "y_predsvr_poly = svr_poly.predict(X_test)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test, y_predsvr_rbf))\n",
    "print('RMs score: %.2f' % rms)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test, y_predsvr_lin))\n",
    "print('RMs score: %.2f' % rms)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test, y_predsvr_poly))\n",
    "print('RMs score: %.2f' % rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ansjin\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "appNames = [mainServiceName]\n",
    "df = df.sort_values(['pod_util_cpu_avg'])\n",
    "df_X = df[['pod_util_cpu_avg','pod_cpu_limit','pod_mem_limit', 'num_pods', 'requests_duration_mean']].values\n",
    "df_Y = df[['requests']].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size=0.33, random_state=42)\n",
    "\n",
    "svr_rbf = SVR(kernel= 'rbf', C= 1e3, gamma= 0.1) # defining the support vector regression models\n",
    "svr_lin = SVR(kernel= 'linear', C= 1e3)\n",
    "svr_poly = SVR(kernel= 'poly', C= 1e3, degree= 2)\n",
    "svr_rbf.fit(X_train, y_train) # fitting the data points in the models\n",
    "svr_lin.fit(X_train, y_train)\n",
    "svr_poly.fit(X_train, y_train)\n",
    "\n",
    "y_predsvr_rbf = svr_rbf.predict(X_test)\n",
    "y_predsvr_lin = svr_lin.predict(X_test)\n",
    "y_predsvr_poly = svr_poly.predict(X_test)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test, y_predsvr_rbf))\n",
    "print('RMs score: %.2f' % rms)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test, y_predsvr_lin))\n",
    "print('RMs score: %.2f' % rms)\n",
    "\n",
    "rms = sqrt(mean_squared_error(y_test, y_predsvr_poly))\n",
    "print('RMs score: %.2f' % rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.17\n",
      "Variance score: 0.79\n",
      "Train Mean squared error: 0.17\n",
      "RMs score: 0.41\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-34-94e85addf0ee>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-34-94e85addf0ee>\"\u001b[1;36m, line \u001b[1;32m35\u001b[0m\n\u001b[1;33m    return regr, rms\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "#train-replicas\n",
    "\n",
    "filename = \"/app/training/preTrainedReplicas/\"+appType+\"/\"+appName+\"/\"+mainServiceName+\"/\"+instanceFamily+\"/\"+\"trained.sav\"\n",
    "appNames = [mainServiceName]\n",
    "df = df.sort_values(['pod_util_cpu_avg'])\n",
    "df_X = df[['pod_util_cpu_avg','pod_cpu_limit','pod_mem_limit', 'requests', 'requests_duration_mean']].values\n",
    "df_Y = df[['num_pods']].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size=0.33, random_state=42)\n",
    "polybig_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "std_scaler = StandardScaler()\n",
    "lin_reg = linear_model.LinearRegression()\n",
    "regr = Pipeline([\n",
    "    (\"poly_features\", polybig_features),\n",
    "    (\"std_scaler\", std_scaler),\n",
    "    (\"lin_reg\", lin_reg),\n",
    "])\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "#print ('Test score %.2f', regr.score(X_test, y_test) )\n",
    "print(\"Train Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, regr.predict(X_train)))\n",
    "rms = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('RMs score: %.2f' % rms)\n",
    "return regr, rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brute-force-training-prediction\n",
    "numcoresUtil = urlparse.parse_qs(parsed.query)['numcoresutil'][0]\n",
    "numcoresLimit = urlparse.parse_qs(parsed.query)['numcoreslimit'][0]\n",
    "nummemLimit = urlparse.parse_qs(parsed.query)['nummemlimit'][0]\n",
    "replicas = 1\n",
    "\n",
    "filename = \"/app/training/preTrained/\"+appType+\"/\"+appName+\"/\"+mainServiceName+\"/\"+instanceFamily+\"/\"+\"trained.sav\"\n",
    "print(filename)\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "val = [[float(numcoresUtil),float(numcoresLimit),float(nummemLimit),float(replicas),float(requestDuration)]]\n",
    "predict = loaded_model.predict(val)\n",
    "output=\"\"+str(predict[0][0])\n",
    "print (predict)\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-replicas-prediction \n",
    "numcoresUtil = urlparse.parse_qs(parsed.query)['numcoresutil'][0]\n",
    "numcoresLimit = urlparse.parse_qs(parsed.query)['numcoreslimit'][0]\n",
    "nummemLimit = urlparse.parse_qs(parsed.query)['nummemlimit'][0]\n",
    "msc = urlparse.parse_qs(parsed.query)['msc'][0]   \n",
    "filename = \"/app/training/preTrainedReplicas/\"+appType+\"/\"+appName+\"/\"+mainServiceName+\"/\"+instanceFamily+\"/\"+\"trained.sav\"\n",
    "print(filename)\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "val = [[float(numcoresUtil),float(numcoresLimit),float(nummemLimit),float(msc),float(requestDuration)]]\n",
    "predict = loaded_model.predict(val)\n",
    "output=\"\"+str(predict[0][0])\n",
    "print (predict)\n",
    "print (output)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smart-test-train \n",
    "folderName = urlparse.parse_qs(parsed.query)['containerName'][0]\n",
    "appNames = [mainServiceName]\n",
    "folderNames = [folderName]\n",
    "filename = \"/app/training/smartTest/\"+appType+\"/\"+appName+\"/\"+mainServiceName+\"/\"+instanceFamily+\"/\"+folderName+\".sav\"\n",
    "df = df.sort_values(['pod_util_cpu_avg'])\n",
    "df = df.head(50)\n",
    "if(appType == \"compute\"):\n",
    "    df_X = df[['pod_util_cpu_avg','requests_duration_mean']].values\n",
    "else:\n",
    "    df_X = df[['pod_util_cpu_avg','requests_duration_mean']].values\n",
    "\n",
    "df_Y = df[['requests']].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size=0.33, random_state=42)\n",
    "polybig_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "std_scaler = StandardScaler()\n",
    "lin_reg = linear_model.LinearRegression()\n",
    "regr = Pipeline([\n",
    "    (\"poly_features\", polybig_features),\n",
    "    (\"std_scaler\", std_scaler),\n",
    "    (\"lin_reg\", lin_reg),\n",
    "])\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "#print ('Test score %.2f', regr.score(X_test, y_test) )\n",
    "print(\"Train Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, regr.predict(X_train)))\n",
    "rms = sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('RMs score: %.2f' % rms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smart-test-train prediction\n",
    "numcoresUtil = urlparse.parse_qs(parsed.query)['numcoresutil'][0]\n",
    "numcoresLimit = urlparse.parse_qs(parsed.query)['numcoreslimit'][0]\n",
    "nummemLimit = urlparse.parse_qs(parsed.query)['nummemlimit'][0]\n",
    "folderName = urlparse.parse_qs(parsed.query)['containerName'][0]\n",
    "filename = \"/app/training/smartTest/\"+appType+\"/\"+appName+\"/\"+mainServiceName+\"/\"+instanceFamily+\"/\"+folderName+\".sav\"\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "val = [[]]\n",
    "if(appType==\"compute\"):\n",
    "    val = [[float(numcoresUtil),float(requestDuration)]]\n",
    "else:\n",
    "    val = [[float(numcoresUtil),float(requestDuration)]]\n",
    "\n",
    "predict = loaded_model.predict(val)\n",
    "output=\"\"+str(predict[0][0])\n",
    "print (predict)\n",
    "print (output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #actual trn\n",
    "folderName = urlparse.parse_qs(parsed.query)['containerName'][0]\n",
    "appNames = [mainServiceName]\n",
    "folderNames = [folderName]\n",
    "df = getAndCombineAllDbs(host, port, username, password,appNames, folderNames)\n",
    "actualValDf= df.sort_values(['requests'])\n",
    "final_df = actualValDf[actualValDf.requests_duration_mean < float(requestDuration)]\n",
    "val = float(final_df.tail(1).requests)\n",
    "output=\"\"+str(val)\n",
    "print (val)\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
